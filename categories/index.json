[{"content":" Deep Dive into Marine Science \u0026amp; Deep Learning Host 1: Welcome back to the Deep Dive. Today we\u0026rsquo;re wading into some really deep water. We\u0026rsquo;re talking about marine science, fish populations, and a pretty big and, honestly, worrying trend.\nHost 2: It\u0026rsquo;s a critical topic. We\u0026rsquo;re seeing this consistent pattern where human activity—you know, everything from fishing to climate change—is causing fish to mature faster and at smaller sizes. The whole biosphere is shrinking.\nHost 1: Right. And if you\u0026rsquo;re trying to manage a fishery or you just care about ocean health, you need to track these changes. You have to know the basics: how fast are they growing? When do they mature?\nHost 2: And that\u0026rsquo;s where the old methods have, well, they\u0026rsquo;ve started to fall short. For a long time, ecologists have relied on what are called mean regression models.\nHost 1: So they\u0026rsquo;re modeling the \u0026ldquo;average\u0026rdquo; fish.\nHost 2: Exactly. Just the average. And that\u0026rsquo;s the danger. By focusing only on the middle, you completely miss what scientists call heterogeneity.\nHost 1: The individual variation.\nHost 2: The individual variation. You miss the super fast growers and the really big, late-maturing ones that are so important for reproduction.\nHost 1: It sounds a bit like trying to understand a country\u0026rsquo;s economy just by looking at the average income. You\u0026rsquo;d miss all the nuance, right? The poverty and the billionaires.\nHost 2: That\u0026rsquo;s a perfect analogy. The average can hide these critical shifts that are happening at the edges. And those edges are often where you first see a population adapting or heading for disaster.\nHost 1: Okay, so let\u0026rsquo;s unpack that. How do we move beyond the average and get the full story without it becoming overwhelmingly complex?\nHost 2: Well, the traditional tool for this, for maturation, has been the Probabilistic Maturation Reaction Norm (PMRN).\nHost 1: PMRN. Okay.\nHost 2: It\u0026rsquo;s basically a statistical model, a logistic regression, that calculates the probability that an average fish will mature based on its age and size.\nHost 1: That sounds pretty solid on the surface. So what\u0026rsquo;s the fatal flaw when you apply it to a real, messy population?\nHost 2: The flaw is this core assumption that the population is, well, uniform. It assumes that both mature and immature fish of the same age are growing at the same rate.\nHost 1: But that\u0026rsquo;s not how it works in reality.\nHost 2: Not at all. You can have two one-year-old fish: one tiny and one huge because of their different growth history. The PMRN really struggles to separate that growth variation from the actual decision to mature.\nHost 1: So it\u0026rsquo;s forcing a simple model onto a really complex biological reality.\nHost 2: Precisely. And that\u0026rsquo;s why this new research explores something different: Quantile Regression (QR).\nHost 1: And QR moves beyond the average.\nHost 2: It does. Instead of just modeling the mean—the 50th percentile—QR lets you model relationships across the whole distribution: the 10th percentile, the 50th, the 90th, all at the same time.\nHost 1: So to go back to my analogy, you\u0026rsquo;re not just seeing the average income anymore; you\u0026rsquo;re seeing the income trajectory for the poorest 10% versus the richest 10%.\nHost 2: Exactly. You can finally see how, say, a change in temperature affects the slowest growers differently than the fastest growers. You see the heterogeneous effects.\nHost 1: So ecologists have used this before?\nHost 2: They have, for some basic things like animal growth. But the big step forward, the one we\u0026rsquo;re looking at today, is combining QR with Deep Neural Networks.\nHost 1: The deep learning leap. Why bring deep learning into it? Is it just about more computer power?\nHost 2: It\u0026rsquo;s more about flexibility. Traditional models often force you to assume a specific mathematical shape for the data, like an exponential curve. A deep quantile framework, on the other hand, lets the DNN learn those complex, non-linear patterns directly from the data without those rigid assumptions.\nHost 1: So you get better predictions.\nHost 2: You get much better predictive accuracy, especially with these huge, messy ecological datasets.\nHost 1: So we\u0026rsquo;re taking the statistical idea of quantile regression and supercharging it with the flexibility of deep learning. Okay, let\u0026rsquo;s get into the new tools this research developed.\nHost 2: They came up with two core models. The first one is the Deep Quantile Growth Model (DQGM). And this one, as the name suggests, is all about growth.\nHost 1: Okay, what makes it \u0026ldquo;deep\u0026rdquo;?\nHost 2: It turns a classic growth model into a multi-output neural network, and each of those outputs corresponds to a specific quantile of the population.\nHost 1: I see. So one output for the 10th percentile growth curve, another for the 20th, and so on.\nHost 2: That\u0026rsquo;s it. And the key is that it doesn\u0026rsquo;t assume what that growth curve should look like beforehand. It lets the data define the growth trajectory at every single level, which gives you this incredibly rich picture of variation.\nHost 1: And the second model handles maturation.\nHost 2: Correct. That\u0026rsquo;s the Deep Binary Quantile Maturation Model (DBQMM). It has to be binary because, well, an individual is either mature or immature.\nHost 1: Right. And this new model gives us a new concept: Quantile Maturation Reaction Norms (QMRNs). How is a QMRN different from the old PMRN?\nHost 2: It\u0026rsquo;s a fundamental shift. A PMRN gives you a single line—the probability for the \u0026ldquo;average\u0026rdquo; fish. A QMRN gives you a whole band of lines.\nHost 1: One for each quantile.\nHost 2: One for each quantile. So you\u0026rsquo;re not just seeing the middle anymore; you\u0026rsquo;re seeing the entire spread, the whole distribution of maturation strategies in the population.\nHost 1: And how are they calculated? It sounds more complex.\nHost 2: It is. The old PMRNs come from simple logistic functions. The QMRNs are derived from what are called Learnt Latent Functions inside the deep learning model. It\u0026rsquo;s a more data-driven and, yeah, a more computationally intense way to get there.\nHost 1: And they created an index to measure this spread, right? The QMRN width.\nHost 2: They did, the \u0026ldquo;dollar dollars.\u0026rdquo; It\u0026rsquo;s a very neat way to put a single number on how variable the population is. It measures the distance between the 25th and 75th percentile curves. So if that width is really wide, you\u0026rsquo;ve got a highly diverse population. If it\u0026rsquo;s narrow, they\u0026rsquo;re all maturing around the same size.\nHost 1: So when they tested this on simulated data, did all this complexity actually pay off?\nHost 2: It absolutely did. The QMRNs were far more reliable and robust, especially when it came to capturing the extreme values—the outliers.\nHost 1: And what did that mean for the old model?\nHost 2: It showed that the traditional PMRN width was always smaller than the QMRN width.\nHost 1: Wait, so the old method was consistently underestimating how much variation there really was.\nHost 2: That\u0026rsquo;s the key takeaway. It was giving us a simplified, maybe even a dangerously optimistic view of the population\u0026rsquo;s diversity. The deep learning approach is just much better at mapping out those extremes where all the interesting stuff happens.\nHost 1: Here\u0026rsquo;s where it gets really interesting. Let\u0026rsquo;s move from the simulation to the real world, to these largehead hairtail populations off the coast of Taiwan.\nHost 2: Right, Trichiurus japonicus. The researchers looked at data from 2013 to 2015 from two very different spots. There was Site K on the cooler northern coast—\u0026ldquo;K for K-ool\u0026rdquo;—and Site T on the warmer southern coast—\u0026ldquo;T for Tropical.\u0026rdquo; They wanted to see if the environment and fishing pressure were driving different life histories.\nHost 1: And were they?\nHost 2: The difference was stark. At the warmer Site T, the fish were maturing much, much earlier. The average age for maturity was just over one year.\nHost 1: Just over a year. Wow.\nHost 2: And they were maturing at a smaller size, around 50 grams. In fact, at Site T, 100% of the individuals were mature before they even hit age two.\nHost 1: So a real \u0026ldquo;live fast, die young\u0026rdquo; strategy. What about up north at the cooler Site K?\nHost 2: It was a completely different story. There, they matured almost half a year later at 1.55 years and at a much larger size, over 82 grams.\nHost 1: That\u0026rsquo;s a huge difference in weight.\nHost 2: It is. And at Site K, only about 80% of the population was mature before age two. So, you know, a slower, bigger strategy. The statistics confirmed it was a very real, significant difference.\nHost 1: And that fits with what we know about biology, right?\nHost 2: Yeah, the Temperature-Size Rule, where cooler water often leads to slower growth but a larger final size. It aligns perfectly. And what\u0026rsquo;s more, the deep DBQMM model proved it was the better tool. It predicted the maturation status at Site K with 88% accuracy. The old logistic model only managed 82%.\nHost 1: Okay, but here\u0026rsquo;s the part I found fascinating: it was the growth patterns. The simple mean-based models actually suggested that the fish at the cooler Site K had a higher average growth rate.\nHost 2: I know, it seems totally counter-intuitive. And that\u0026rsquo;s a perfect example of the mean hiding the details.\nHost 1: So what did the DQGM—the quantile model—reveal?\nHost 2: It clarified the whole story. It showed that the juveniles at the warmer Site T have this incredibly rapid burst of growth in their first year—they just shoot out of the gate.\nHost 1: But then it slows down.\nHost 2: Exactly. Their growth slows way down as adults, and the fish from the colder Site K eventually overtake them and reach a much larger final size. That initial growth burst at the warmer site was completely invisible to the mean model.\nHost 1: And that has to have implications for fisheries, right? If the fish at the warmer Site T are growing fast early and maturing small, it suggests they\u0026rsquo;re under incredible fishing pressure.\nHost 2: It absolutely does. The researchers believe this early maturation might be an evolutionary response to intense, size-selective harvesting. It\u0026rsquo;s exactly the kind of nuance a fishery manager needs to see to regulate things properly.\nHost 1: And that brings us to why this all really matters. Why we should care so much about the fish at the edges of the curve.\nHost 2: It all comes down to one word: Fecundity. That\u0026rsquo;s reproductive output. And it doesn\u0026rsquo;t scale linearly with body size; it scales allometrically.\nHost 1: Okay, hold on. Allometrically. Let\u0026rsquo;s break that down for a second. That means it\u0026rsquo;s not a one-to-one relationship, right?\nHost 2: Right, it\u0026rsquo;s disproportionate. A fish that\u0026rsquo;s twice as big can produce, you know, maybe four or five times the eggs. These few really large individuals in the upper quantiles—they are the reproductive powerhouses of the population.\nHost 1: So if your model only looks at the average-sized fish, you are completely ignoring the most important contributors to the next generation.\nHost 2: You risk significantly—and I mean significantly—underestimating the population\u0026rsquo;s reproductive potential. You could have one giant fish that produces more eggs than a hundred small ones. If your model doesn\u0026rsquo;t see that giant, you\u0026rsquo;re flying blind.\nHost 1: So these new models are clearly more accurate, they\u0026rsquo;re more robust, but all this complexity—all this deep learning—comes with a cost: the \u0026ldquo;Black Box\u0026rdquo; problem.\nHost 2: Yes, the black box. It\u0026rsquo;s the classic trade-off with machine learning. The neural networks are so complex, it\u0026rsquo;s hard to look inside and understand exactly why they\u0026rsquo;re making a certain prediction. For an ecologist, that loss of interpretability is a real concern.\nHost 1: Is there any way around that? To get the accuracy but also get some of the \u0026ldquo;why\u0026rdquo;?\nHost 2: There are tools. The researchers suggest using something called SHAP values. It\u0026rsquo;s a method that can essentially force the black box to explain itself. It quantifies how much each input, like age or temperature, contributed to the final result.\nHost 1: So it helps you peek inside. What\u0026rsquo;s the other big hurdle for an ecologist who wants to use this?\nHost 2: It\u0026rsquo;s the technical side. Hyper-parameter tuning. Just setting up and optimizing these networks is complex and needs a lot of computational resources. It\u0026rsquo;s a significant barrier for labs that don\u0026rsquo;t have a dedicated machine learning expert on site.\nHost 1: Okay, so we have this tension: we have superior accuracy versus this added complexity. And this leads to the final, really interesting wrinkle in the study.\nHost 2: Right. Because despite the deep quantile model, the QMRN, being demonstrably better at predicting the extremes\u0026hellip; when they ran a standard statistical test, just comparing the population averages, there was no significant difference between the old model and the new one.\nHost 1: So let me get this straight: you have this cutting-edge, complex model that, if you only look at the average, tells you the same thing as the simple old model. But at the same time, it\u0026rsquo;s revealing this crucial, hidden story about the most important individuals that the old model completely missed.\nHost 2: That is the paradox. Our quantile-based deep neural network gives ecologists this much richer, more nuanced view of what\u0026rsquo;s happening. It has lower errors, provides deeper insights\u0026hellip; but its real value isn\u0026rsquo;t in correcting the average. Its value is in showing us that the average was never the whole story to begin with.\nHost 1: So what does this all mean for you, the listener? If the new model doesn\u0026rsquo;t change the official population average, but it reveals that the most reproductively valuable fish are being missed or are facing unique pressure, how do fishery managers justify the massive cost and complexity of integrating this new machine learning system? Are we trading statistical simplicity for a deeper, truer understanding of ecological sustainability? Something to consider as we move into the age of deep learning ecology.\n","description":"Deep quantile regression for growth and maturation reaction norms.","id":1,"section":"showcase","tags":null,"title":"Deep Quantile Maturation Reaction Norms","uri":"https://www.metasphinx.com/showcase/papercode/deepqmrn/"},{"content":"\nIn oceans across the globe, a silent, pervasive change is underway: fish are getting smaller. This isn\u0026rsquo;t a random fluctuation; it\u0026rsquo;s a consistent, troubling pattern driven by a warming climate and the relentless pressure of our fishing nets. This raises a critical question: how do we accurately track these fundamental changes to animal populations?\nFor decades, the standard approach has often involved focusing on the \u0026ldquo;average\u0026rdquo; individual to understand population trends. However, this method can overlook crucial details and variations within a population, potentially hiding a more complex reality. Now, a new approach combining the power of deep learning with a statistical method called quantile regression offers a more nuanced and powerful way to understand what is truly happening to life in our oceans.\n\u0026ldquo;Average\u0026rdquo; Is a Myth—And It\u0026rsquo;s Hiding a Dangerous Truth\nTraditional scientific models often rely on \u0026ldquo;mean regression,\u0026rdquo; a method that calculates the average response to understand trends like growth rates. But imagine trying to understand the academic performance of a school by only looking at the average grade. You\u0026rsquo;d miss the story of the struggling students who need help and the high-achievers who are pushing the boundaries.\nQuantile regression is like looking at the full report card. Instead of focusing only on the average, this method models relationships across the entire distribution of a population. It allows scientists to analyze the bottom 10%, the top 10%, and every slice in between, revealing a complete and often surprising picture. They can compare the slowest-growing 10% of individuals to the median individual (the 50th percentile) or the fastest-growing 90%, providing a much deeper understanding of how different parts of a population are responding to environmental pressures.\nThe real-world implication of this difference is critical. As new research highlights, focusing only on the average can lead to flawed conclusions with serious consequences for conservation.\nRelying solely on the median growth curve fails to capture the disproportionate contribution of larger, highly fecund individuals, potentially leading to a significant underestimation of the population\u0026rsquo;s total reproductive potential.\nIn short, if we only look at the \u0026ldquo;average\u0026rdquo; fish, we completely miss the vital role that the largest, most fertile individuals play in sustaining the population. This underestimation could lead to the mismanagement of fisheries, threatening their long-term survival.\nDeep Learning Becomes a New Microscope for Ecologists\nThe new method presented in the research uses deep neural networks—a form of AI—to power this advanced quantile regression analysis. A key advantage of this AI-driven approach is that it removes the reliance on scientists having to pre-select a specific biological growth function or make prior assumptions about how the data should behave.\nThis is a profound shift. Previously, ecologists had to make an educated guess about the mathematical shape of a fish\u0026rsquo;s growth curve—assuming it was exponential, for instance. If that assumption was wrong, the entire model could be flawed. The deep learning framework, however, makes no such assumptions. It learns the true, often complex, growth patterns directly from the data, acting as a more honest and flexible microscope.\nIn the study, this method achieved higher predictive accuracy than traditional approaches. The researchers developed two specific models to achieve this:\n The deep quantile growth model (deep QGM): To map out the full range of growth trajectories, from the slowest to the fastest-growing individuals. The deep binary quantile maturation model (deep BQMM): To understand the different ages and sizes at which fish across the population become ready to reproduce.  These sophisticated new tools give ecologists a clearer, more detailed picture of population dynamics than was previously possible.\nA Tale of Two Coasts: Fish Life Stories Are Being Rewritten\nTo test their framework, researchers applied it to a case study of the largehead hairtail (Trichiurus japonicus), a commercially important fish, in two distinct locations off the coasts of Taiwan Island: the cooler northern site (K) and the warmer southern site (T). The results revealed that the life stories of these fish are being dramatically rewritten by their environments.\nIn the cooler northern waters, the largehead hairtail can afford to play the long game. They mature later, at an average of 1.55 years, and ultimately reach a larger, more fecund size.\nBut in the warmer, heavily fished southern waters, it\u0026rsquo;s a race against the net. These fish mature much earlier, at just 1.08 years, and at a fraction of the size. These differences align with established ecological principles like the \u0026ldquo;temperature-size rule,\u0026rdquo; which predicts that organisms in cooler climates often grow slower but reach larger adult sizes. However, the study points to another major factor at play: intense, size-selective fishing. The research suggests that heavy fishing pressure at the southern site may have \u0026ldquo;induced evolutionary changes, resulting in earlier maturation at smaller sizes\u0026rdquo;—a classic sign of fisheries-induced evolution, where the pressure of the nets favors fish that reproduce before they can be caught.\nConclusion: A More Complex, Complete Picture\nBy moving beyond simple averages and embracing advanced tools like deep quantile regression, we can gain a richer, more accurate understanding of how populations are responding to mounting environmental pressures. This new framework allows scientists to move from simple point estimates to a \u0026ldquo;probabilistic understanding of population structure and dynamics,\u0026rdquo; which is a major leap forward in ecological science.\nThis ability to see the full spectrum of responses within a population is essential for developing effective conservation strategies. This granular view could allow fishery managers to move beyond one-size-fits-all regulations, tailoring quotas to protect the fast-growing, highly fertile subgroups that are critical to a population’s resilience, or identifying specific areas where younger fish are maturing dangerously early. It leaves us with a final, crucial question to consider: if the \u0026ldquo;average\u0026rdquo; fish is a fiction, what other critical ecological truths are we missing by looking only at the mean?\n","description":"Beyond the average: How AI is revealing the hidden truths of ocean life?","id":5,"section":"publication","tags":null,"title":"Deep quantile regression for growth and maturation reaction norms","uri":"https://www.metasphinx.com/publication/article/2025_deep_quantile_regression_for_growth_and_maturation_reaction_norms/"},{"content":" deep QGM Network Architecture  digraph {\rgraph [size=\u0026quot;22.05,22.05\u0026quot;]\rnode [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]\r139945457199760 [label=\u0026quot;\r(1, 9)\u0026quot; fillcolor=darkolivegreen1]\r139945434913568 [label=CatBackward0]\r139945434913664 -\u0026gt; 139945434913568\r139945434913664 [label=AddmmBackward0]\r139945434914000 -\u0026gt; 139945434913664\r139945437824480 [label=\u0026quot;outputs.0.bias\r(1)\u0026quot; fillcolor=lightblue]\r139945437824480 -\u0026gt; 139945434914000\r139945434914000 [label=AccumulateGrad]\r139945434913952 -\u0026gt; 139945434913664\r139945434913952 [label=ReluBackward0]\r139945434914096 -\u0026gt; 139945434913952\r139945434914096 [label=AddmmBackward0]\r139945434914240 -\u0026gt; 139945434914096\r139945459033088 [label=\u0026quot;fc2.bias\r(10)\u0026quot; fillcolor=lightblue]\r139945459033088 -\u0026gt; 139945434914240\r139945434914240 [label=AccumulateGrad]\r139945434914192 -\u0026gt; 139945434914096\r139945434914192 [label=ReluBackward0]\r139945434914336 -\u0026gt; 139945434914192\r139945434914336 [label=AddmmBackward0]\r139945434914528 -\u0026gt; 139945434914336\r139945457179680 [label=\u0026quot;fc1.bias\r(20)\u0026quot; fillcolor=lightblue]\r139945457179680 -\u0026gt; 139945434914528\r139945434914528 [label=AccumulateGrad]\r139945434914480 -\u0026gt; 139945434914336\r139945434914480 [label=TBackward0]\r139945434914576 -\u0026gt; 139945434914480\r139945457214224 [label=\u0026quot;fc1.weight\r(20, 1)\u0026quot; fillcolor=lightblue]\r139945457214224 -\u0026gt; 139945434914576\r139945434914576 [label=AccumulateGrad]\r139945434913328 -\u0026gt; 139945434914096\r139945434913328 [label=TBackward0]\r139945434914624 -\u0026gt; 139945434913328\r139945454148400 [label=\u0026quot;fc2.weight\r(10, 20)\u0026quot; fillcolor=lightblue]\r139945454148400 -\u0026gt; 139945434914624\r139945434914624 [label=AccumulateGrad]\r139945434913904 -\u0026gt; 139945434913664\r139945434913904 [label=TBackward0]\r139945434914432 -\u0026gt; 139945434913904\r139945437824400 [label=\u0026quot;outputs.0.weight\r(1, 10)\u0026quot; fillcolor=lightblue]\r139945437824400 -\u0026gt; 139945434914432\r139945434914432 [label=AccumulateGrad]\r139945434913616 -\u0026gt; 139945434913568\r139945434913616 [label=AddmmBackward0]\r139945434914768 -\u0026gt; 139945434913616\r139945434909808 [label=\u0026quot;outputs.1.bias\r(1)\u0026quot; fillcolor=lightblue]\r139945434909808 -\u0026gt; 139945434914768\r139945434914768 [label=AccumulateGrad]\r139945434913952 -\u0026gt; 139945434913616\r139945434914288 -\u0026gt; 139945434913616\r139945434914288 [label=TBackward0]\r139945434914720 -\u0026gt; 139945434914288\r139945435153648 [label=\u0026quot;outputs.1.weight\r(1, 10)\u0026quot; fillcolor=lightblue]\r139945435153648 -\u0026gt; 139945434914720\r139945434914720 [label=AccumulateGrad]\r139945434913520 -\u0026gt; 139945434913568\r139945434913520 [label=AddmmBackward0]\r139945434912512 -\u0026gt; 139945434913520\r139945434909968 [label=\u0026quot;outputs.2.bias\r(1)\u0026quot; fillcolor=lightblue]\r139945434909968 -\u0026gt; 139945434912512\r139945434912512 [label=AccumulateGrad]\r139945434913952 -\u0026gt; 139945434913520\r139945434914144 -\u0026gt; 139945434913520\r139945434914144 [label=TBackward0]\r139945434914672 -\u0026gt; 139945434914144\r139945434909888 [label=\u0026quot;outputs.2.weight\r(1, 10)\u0026quot; fillcolor=lightblue]\r139945434909888 -\u0026gt; 139945434914672\r139945434914672 [label=AccumulateGrad]\r139945434913280 -\u0026gt; 139945434913568\r139945434913280 [label=AddmmBackward0]\r139945434914384 -\u0026gt; 139945434913280\r139945434910128 [label=\u0026quot;outputs.3.bias\r(1)\u0026quot; fillcolor=lightblue]\r139945434910128 -\u0026gt; 139945434914384\r139945434914384 [label=AccumulateGrad]\r139945434913952 -\u0026gt; 139945434913280\r139945434914048 -\u0026gt; 139945434913280\r139945434914048 [label=TBackward0]\r139945435037808 -\u0026gt; 139945434914048\r139945434910048 [label=\u0026quot;outputs.3.weight\r(1, 10)\u0026quot; fillcolor=lightblue]\r139945434910048 -\u0026gt; 139945435037808\r139945435037808 [label=AccumulateGrad]\r139945434913472 -\u0026gt; 139945434913568\r139945434913472 [label=AddmmBackward0]\r139945435038000 -\u0026gt; 139945434913472\r139945434910288 [label=\u0026quot;outputs.4.bias\r(1)\u0026quot; fillcolor=lightblue]\r139945434910288 -\u0026gt; 139945435038000\r139945435038000 [label=AccumulateGrad]\r139945434913952 -\u0026gt; 139945434913472\r139945435037760 -\u0026gt; 139945434913472\r139945435037760 [label=TBackward0]\r139945435037952 -\u0026gt; 139945435037760\r139945434910208 [label=\u0026quot;outputs.4.weight\r(1, 10)\u0026quot; fillcolor=lightblue]\r139945434910208 -\u0026gt; 139945435037952\r139945435037952 [label=AccumulateGrad]\r139945434913376 -\u0026gt; 139945434913568\r139945434913376 [label=AddmmBackward0]\r139945435038144 -\u0026gt; 139945434913376\r139945434910448 [label=\u0026quot;outputs.5.bias\r(1)\u0026quot; fillcolor=lightblue]\r139945434910448 -\u0026gt; 139945435038144\r139945435038144 [label=AccumulateGrad]\r139945434913952 -\u0026gt; 139945434913376\r139945435037904 -\u0026gt; 139945434913376\r139945435037904 [label=TBackward0]\r139945435038096 -\u0026gt; 139945435037904\r139945434910368 [label=\u0026quot;outputs.5.weight\r(1, 10)\u0026quot; fillcolor=lightblue]\r139945434910368 -\u0026gt; 139945435038096\r139945435038096 [label=AccumulateGrad]\r139945434913712 -\u0026gt; 139945434913568\r139945434913712 [label=AddmmBackward0]\r139945435038288 -\u0026gt; 139945434913712\r139945434910608 [label=\u0026quot;outputs.6.bias\r(1)\u0026quot; fillcolor=lightblue]\r139945434910608 -\u0026gt; 139945435038288\r139945435038288 [label=AccumulateGrad]\r139945434913952 -\u0026gt; 139945434913712\r139945435038048 -\u0026gt; 139945434913712\r139945435038048 [label=TBackward0]\r139945435038240 -\u0026gt; 139945435038048\r139945434910528 [label=\u0026quot;outputs.6.weight\r(1, 10)\u0026quot; fillcolor=lightblue]\r139945434910528 -\u0026gt; 139945435038240\r139945435038240 [label=AccumulateGrad]\r139945434913760 -\u0026gt; 139945434913568\r139945434913760 [label=AddmmBackward0]\r139945435038432 -\u0026gt; 139945434913760\r139945434992784 [label=\u0026quot;outputs.7.bias\r(1)\u0026quot; fillcolor=lightblue]\r139945434992784 -\u0026gt; 139945435038432\r139945435038432 [label=AccumulateGrad]\r139945434913952 -\u0026gt; 139945434913760\r139945435038192 -\u0026gt; 139945434913760\r139945435038192 [label=TBackward0]\r139945435038384 -\u0026gt; 139945435038192\r139945434992704 [label=\u0026quot;outputs.7.weight\r(1, 10)\u0026quot; fillcolor=lightblue]\r139945434992704 -\u0026gt; 139945435038384\r139945435038384 [label=AccumulateGrad]\r139945434913808 -\u0026gt; 139945434913568\r139945434913808 [label=AddmmBackward0]\r139945435038576 -\u0026gt; 139945434913808\r139945434992944 [label=\u0026quot;outputs.8.bias\r(1)\u0026quot; fillcolor=lightblue]\r139945434992944 -\u0026gt; 139945435038576\r139945435038576 [label=AccumulateGrad]\r139945434913952 -\u0026gt; 139945434913808\r139945435038336 -\u0026gt; 139945434913808\r139945435038336 [label=TBackward0]\r139945435038528 -\u0026gt; 139945435038336\r139945434992864 [label=\u0026quot;outputs.8.weight\r(1, 10)\u0026quot; fillcolor=lightblue]\r139945434992864 -\u0026gt; 139945435038528\r139945435038528 [label=AccumulateGrad]\r139945434913568 -\u0026gt; 139945457199760\r}\r deep BQMM Network Architecture  digraph {\rgraph [size=\u0026quot;12,12\u0026quot;]\rnode [align=left fontname=\u0026quot;Times New Roman\u0026quot; fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]\r139794723974976 [label=\u0026quot;\r(1, 9)\u0026quot; fillcolor=darkolivegreen1]\r139794723952576 [label=AddmmBackward0]\r139794784143344 -\u0026gt; 139794723952576\r139794723973296 [label=\u0026quot;outputs.bias\r(9)\u0026quot; fillcolor=lightblue]\r139794723973296 -\u0026gt; 139794784143344\r139794784143344 [label=AccumulateGrad]\r139794784143248 -\u0026gt; 139794723952576\r139794784143248 [label=LeakyReluBackward0]\r139798439809328 -\u0026gt; 139794784143248\r139798439809328 [label=AddmmBackward0]\r139798439809472 -\u0026gt; 139798439809328\r139794784064912 [label=\u0026quot;fc2.bias\r(10)\u0026quot; fillcolor=lightblue]\r139794784064912 -\u0026gt; 139798439809472\r139798439809472 [label=AccumulateGrad]\r139798439809760 -\u0026gt; 139798439809328\r139798439809760 [label=LeakyReluBackward0]\r139798439809136 -\u0026gt; 139798439809760\r139798439809136 [label=AddmmBackward0]\r139798439635456 -\u0026gt; 139798439809136\r139794723973536 [label=\u0026quot;fc1.bias\r(100)\u0026quot; fillcolor=lightblue]\r139794723973536 -\u0026gt; 139798439635456\r139798439635456 [label=AccumulateGrad]\r139798439635552 -\u0026gt; 139798439809136\r139798439635552 [label=TBackward0]\r139798439635648 -\u0026gt; 139798439635552\r139794723973376 [label=\u0026quot;fc1.weight\r(100, 1)\u0026quot; fillcolor=lightblue]\r139794723973376 -\u0026gt; 139798439635648\r139798439635648 [label=AccumulateGrad]\r139798439809664 -\u0026gt; 139798439809328\r139798439809664 [label=TBackward0]\r139798439635504 -\u0026gt; 139798439809664\r139794742617008 [label=\u0026quot;fc2.weight\r(10, 100)\u0026quot; fillcolor=lightblue]\r139794742617008 -\u0026gt; 139798439635504\r139798439635504 [label=AccumulateGrad]\r139794726867440 -\u0026gt; 139794723952576\r139794726867440 [label=TBackward0]\r139798439809280 -\u0026gt; 139794726867440\r139798507380880 [label=\u0026quot;outputs.weight\r(9, 10)\u0026quot; fillcolor=lightblue]\r139798507380880 -\u0026gt; 139798439809280\r139798439809280 [label=AccumulateGrad]\r139794723952576 -\u0026gt; 139794723974976\r}\r","description":"deep quantile regression.","id":6,"section":"blogs","tags":["Python"],"title":"Deep Quantile Regression","uri":"https://www.metasphinx.com/blogs/2025/deep_quantile_regression/"},{"content":"I am Guankui Liu, a Ph.D. candidate at the Deep Sea and Polar Fisheries Research Center, Ocean University of China. My current research focuses on the impacts of dual pressures from fisheries and environmental changes on fishery resources. I am passionate about integrating remote sensing, ecology, and Earth sciences with statistical and machine learning methods to address fisheries-related challenges. Additionally, I am particularly interested in using graphical models and causal inference approaches to evaluate the effectiveness of fisheries management policies.\nI am expected to complete my Ph.D. in the summer of 2025 and am actively seeking a postdoctoral position. \nBelow is a word cloud representation of my research, based on the titles and abstracts of my publications prior to 2025.\n  Code  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  import jieba.analyse import numpy as np import matplotlib.pyplot as plt from PIL import Image from wordcloud import WordCloud, ImageColorGenerator plt.rcParams[\u0026#34;figure.dpi\u0026#34;] = 1200 with open(\u0026#39;Mypaper_abstract.txt\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: text = f.read() freq = jieba.analyse.extract_tags(text, topK=100, withWeight=True) freq = {i[0]: i[1] for i in freq} mask = np.array(Image.open(\u0026#34;./Shark_color_mask.png\u0026#34;)) wc = WordCloud(mask=mask, font_path=\u0026#39;./Hiragino.ttf\u0026#39;, mode=\u0026#39;RGBA\u0026#39;, background_color=None).generate_from_frequencies(freq) image_colors = ImageColorGenerator(mask) wc.recolor(color_func=image_colors) plt.imshow(wc) plt.axis(\u0026#34;off\u0026#34;) plt.show()      ","description":"About Page","id":11,"section":"","tags":null,"title":"Who am I","uri":"https://www.metasphinx.com/about/"},{"content":"Makie.jl is the front end package that defines all plotting functions required to create plot objects. To convert these plot objects to an image, three main back ends which concretely implement all abstract ren-dering capabilities defined in Makie.\n CairoMakie.j: a non-interactive 2D publication-quality vector graphics. GLMakie.j: a interactive 2D and 3D plot-ting in standalone GLFW.jl windows. WGLMakie.jl: a WebGL-based interactive 2D and 3D plotting that runs within browsers.  CairoMakie.jl 1 2 3 4 5 6  # save(\u0026#34;filename.pdf\u0026#34;, fig; pt_per_unit=2)  # save(\u0026#34;filename.png\u0026#34;, fig; px_per_unit=0.5)  using CairoMakie CairoMakie.activate!() fig = scatterlines(1:10, 1:10)   Attributes 1 2  fig, ax, pltobj = scatterlines(1:10) pltobj.attributes   Attributes with 15 entries:\rcolor =\u0026gt; RGBA{Float32}(0.0,0.447059,0.698039,1.0)\rcolormap =\u0026gt; viridis\rcolorrange =\u0026gt; Automatic()\rcycle =\u0026gt; [:color]\rinspectable =\u0026gt; true\rlinestyle =\u0026gt; nothing\rlinewidth =\u0026gt; 1.5\rmarker =\u0026gt; circle\rmarkercolor =\u0026gt; Automatic()\rmarkercolormap =\u0026gt; viridis\rmarkercolorrange =\u0026gt; Automatic()\rmarkersize =\u0026gt; 12\rmodel =\u0026gt; Float32[1.0 0.0 0.0 0.0; 0.0 1.0 0.0 0.0; 0.0 0.0 1.0 0.0; 0.0 0.0 0.0 1.0]\rstrokecolor =\u0026gt; black\rstrokewidth =\u0026gt; 0\r1 2 3 4 5  lines(1:10, (1:10).^2; color = :black, linewidth = 2, linestyle = :dash, figure = (; figure_padding = 5, resolution = (600, 400), font = \u0026#34;sans\u0026#34;, backgroundcolor = :grey90, fontsize = 16), axis = (; xlabel = \u0026#34;x\u0026#34;, ylabel = \u0026#34;x²\u0026#34;, title = \u0026#34;title\u0026#34;, xgridstyle = :dash, ygridstyle = :dash)) current_figure()   1 2 3 4 5 6 7  lines(1:10, (1:10).^2; label = \u0026#34;x²\u0026#34;, linewidth = 2, linestyle = nothing, figure = (; figure_padding = 5, resolution = (600, 400), font = \u0026#34;sans\u0026#34;, backgroundcolor = :grey90, fontsize = 16), axis=(; xlabel = \u0026#34;x\u0026#34;, title = \u0026#34;title\u0026#34;, xgridstyle = :dash, ygridstyle = :dash)) scatterlines!(1:10, (10:-1.0:1).^2; label = \u0026#34;Reverse(x)²\u0026#34;) axislegend(\u0026#34;legend\u0026#34;; position = :ct) current_figure()   1 2 3 4 5 6 7  set_theme!(; resolution = (600, 400), backgroundcolor = (:orange, 0.5), fontsize = 16, font=\u0026#34;sans\u0026#34;, Axis = (backgroundcolor = :grey90, xgridstyle = :dash, ygridstyle = :dash), Legend = (bgcolor = (:red, 0.2), framecolor = :dodgerblue)) lines(1:10, (1:10).^2; label = \u0026#34;x²\u0026#34;, linewidth = 2, linestyle = nothing, axis = (; xlabel = \u0026#34;x\u0026#34;, title = \u0026#34;title\u0026#34;)) scatterlines!(1:10, (10:-1.0:1).^2; label = \u0026#34;Reverse(x)²\u0026#34;) axislegend(\u0026#34;legend\u0026#34;; position = :ct) current_figure()   1 2 3 4 5  using Random: seed! seed!(28) xyvals = randn(100, 3) xyvals[1:5, :]   5×3 Matrix{Float64}:\r0.550992 1.27614 -0.659886\r-1.06587 -0.0287242 0.175126\r-0.721591 -1.84423 0.121052\r0.801169 0.862781 -0.221599\r-0.340826 0.0589894 -1.76359\r1 2 3 4 5 6 7 8 9  set_theme!() # reset the default settings of Makie fig, ax, pltobj = scatter(xyvals[:, 1], xyvals[:, 2]; color = xyvals[:, 3], label = \u0026#34;Bubbles\u0026#34;, colormap = :plasma, markersize = 15 * abs.(xyvals[:, 3]), figure = (; resolution = (600, 400)), axis = (; aspect = DataAspect())) limits!(-3, 3, -3, 3) Legend(fig[1, 2], ax, valign = :top) Colorbar(fig[1, 2], pltobj, height = Relative(3 / 4)) fig   Themes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  seed!(123) y =cumsum(randn(6, 6), dims = 2) seed!(13) xv = yv = LinRange(-3, 0.5, 20) matrix =randn(20, 20) function demo_themes(y, xv, yv, matrix) fig, _ = series(y; labels = [\u0026#34;$i\u0026#34; for i = 1:6], markersize=10, color=:Set1, figure = (; resolution=(600, 300)), axis = (; xlabel = \u0026#34;time(s)\u0026#34;, ylabel = \u0026#34;Amplitude\u0026#34;, title = \u0026#34;Measurements\u0026#34;)) hmap = heatmap!(xv, yv, matrix; colormap = :plasma) limits!(-3.1, 8.5, -6, 5.1) axislegend(\u0026#34;legend\u0026#34;; merge = true) Colorbar(fig[1, 2], hmap) fig end with_theme(theme_dark()) do demo_themes(y, xv, yv, matrix) end   1 2 3  with_theme(theme_black()) do demo_themes(y, xv, yv, matrix) end   1 2 3  with_theme(theme_ggplot2()) do demo_themes(y, xv, yv, matrix) end   1 2 3  with_theme(theme_minimal()) do demo_themes(y, xv, yv, matrix) end   1 2 3  with_theme(theme_light()) do demo_themes(y, xv, yv, matrix) end   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  publication_theme() = Theme(fontsize = 16, font = \u0026#34;CMU Serif\u0026#34;, Axis = (xlabelsize = 20, xgridstyle = :dash, ygridstyle = :dash, xtickalign = 1, ytickalign = 1, yticksize = 10, xticksize = 10, xlabelpadding = -5, xlabel = \u0026#34;x\u0026#34;, ylabel = \u0026#34;y\u0026#34;), Legend = (framecolor = (:black, 0.5), bgcolor = (:white, 0.5)), Colorbar = (ticksize = 16, tickalign = 1, spinewidth = 0.5), ) function plot_with_legend_and_colorbar() fig, ax, _ = scatterlines(1:10; label = \u0026#34;line\u0026#34;) hm = heatmap!(ax, LinRange(6, 9, 15), LinRange(2, 5, 15), randn(15, 15); colormap = :Spectral_11) axislegend(\u0026#34;legend\u0026#34;; position = :lt) Colorbar(fig[1, 2], hm, label = \u0026#34;values\u0026#34;) ax.title = \u0026#34;my custom theme\u0026#34; fig end with_theme(plot_with_legend_and_colorbar, publication_theme())   If something needs to be changed after set_theme!(your_theme), we can do it with update_theme!(resolution=(500, 400), fontsize=18), for example. Another approach will be to pass additional arguments to the with_theme function:\n1 2 3 4 5 6  fig = (resolution = (600, 400), figure_padding = 1, backgroundcolor = :grey90) ax = (; aspect = DataAspect(), xlabel = L\u0026#34;x\u0026#34;, ylabel = L\u0026#34;y\u0026#34;) cbar = (; height = Relative(4 / 5)) with_theme(publication_theme(); fig..., Axis = ax, Colorbar = cbar) do plot_with_legend_and_colorbar() end   Using LaTexStrings.jl 1 2 3 4 5 6 7 8 9 10 11 12 13 14  using LaTeXStrings function LaTeX_Strings() x = 0:0.05:4π lines(x, x -\u0026gt; sin(3x) / (cos(x) + 2) / x; label = L\u0026#34;\\frac{\\sin(3x)}{x(\\cos(x)+2)}\u0026#34;, figure = (; resolution = (600, 400)), axis = (; xlabel = L\u0026#34;x\u0026#34;)) lines!(x, x -\u0026gt; cos(x) / x; label = L\u0026#34;\\cos(x)/x\u0026#34;) lines!(x, x -\u0026gt; exp(-x); label = L\u0026#34;e^{−x}\u0026#34;) limits!(-0.5, 13, -0.6, 1.05) axislegend(L\u0026#34;f(x)\u0026#34;) current_figure() end with_theme(LaTeX_Strings, publication_theme())   1 2 3 4 5 6 7 8 9 10 11 12 13  function multiple_lines() x = collect(0:10) fig = Figure(resolution = (600, 400), font = \u0026#34;CMU Serif\u0026#34;) ax = Axis(fig[1, 1], xlabel = L\u0026#34;x\u0026#34;, ylabel = L\u0026#34;f(x,a)\u0026#34;) for i = 0:10 lines!(ax, x, i .* x; label = latexstring(\u0026#34;$(i)x\u0026#34;)) end axislegend(L\u0026#34;f(x)\u0026#34;; position = :lt, nbanks = 2, labelsize = 14) text!(L\u0026#34;f(x,a) = ax\u0026#34;, position = (4, 80)) fig end multiple_lines()   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  function multiple_scatters_and_lines() x = collect(0:10) cycle = Cycle([:color, :linestyle, :marker], covary=true) set_theme!(Lines = (cycle = cycle,), Scatter = (cycle = cycle,)) fig = Figure(resolution = (600, 400), font = \u0026#34;CMU Serif\u0026#34;) ax = Axis(fig[1, 1], xlabel = L\u0026#34;x\u0026#34;, ylabel = L\u0026#34;f(x,a)\u0026#34;) for i in x lines!(ax, x, i .* x; label = latexstring(\u0026#34;$(i)x\u0026#34;)) scatter!(ax, x, i .* x; markersize = 13, strokewidth = 0.25, label = latexstring(\u0026#34;$(i)x\u0026#34;)) end axislegend(L\u0026#34;f(x)\u0026#34;; merge = true, position = :lt, nbanks = 2, labelsize = 14) text!(L\u0026#34;f(x,a) = ax\u0026#34;, position = (4, 80)) set_theme!() # reset to default theme fig end multiple_scatters_and_lines()   Colors and Corlormaps 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  function set_colors_and_cycle() # Epicycloid lines x(r, k, θ) = r * (k .+ 1.0) .* cos.(θ) .- r * cos.((k .+ 1.0) .* θ) y(r, k, θ) = r * (k .+ 1.0) .* sin.(θ) .- r * sin.((k .+ 1.0) .* θ) θ = LinRange(0, 6.2π, 1000) axis = (; xlabel = L\u0026#34;x(\\theta)\u0026#34;, ylabel = L\u0026#34;y(\\theta)\u0026#34;, title = \u0026#34;Epicycloid\u0026#34;, aspect = DataAspect()) figure = (; resolution=(600, 400), font=\u0026#34;CMU Serif\u0026#34;) fig, ax, _ = lines(x(1, 1, θ), y(1, 1, θ); color = \u0026#34;firebrick1\u0026#34;, label = L\u0026#34;1.0\u0026#34;, axis = axis, figure = figure) lines!(ax, x(4, 2, θ), y(4, 2, θ); color = :royalblue1, label = L\u0026#34;2.0\u0026#34;) for k = 2.5:0.5:5.5 lines!(ax, x(2k, k, θ), y(2k, k, θ); label = latexstring(\u0026#34;$(k)\u0026#34;)) #cycle end Legend(fig[1, 2], ax, latexstring(\u0026#34;k, r = 2k\u0026#34;), merge = true) fig end set_colors_and_cycle()   1 2 3 4 5 6 7  figure = (; resolution = (600, 400), font = \u0026#34;CMU Serif\u0026#34;) axis = (; xlabel = L\u0026#34;x\u0026#34;, ylabel = L\u0026#34;y\u0026#34;, aspect = DataAspect()) fig, ax, pltobj = heatmap(rand(20, 20); colorrange = (0, 1), colormap = Reverse(:viridis), axis = axis, figure = figure) Colorbar(fig[1, 2], pltobj, label = \u0026#34;Reverse colormap Sequential\u0026#34;) colsize!(fig.layout, 1, Aspect(1, 1.0)) fig   1 2 3 4 5 6 7 8 9  using ColorSchemes figure = (; resolution = (600, 400), font = \u0026#34;CMU Serif\u0026#34;) axis = (; xlabel = L\u0026#34;x\u0026#34;, ylabel = L\u0026#34;y\u0026#34;, aspect = DataAspect()) fig, ax, pltobj = heatmap(randn(20, 20); colorrange = (-2, 2), colormap = \u0026#34;diverging_rainbow_bgymr_45_85_c67_n256\u0026#34;, highclip = :black, lowclip = :white, axis = axis, figure = figure) Colorbar(fig[1, 2], pltobj, label = \u0026#34;Diverging colormap\u0026#34;) colsize!(fig.layout, 1, Aspect(1, 1.0)) fig   1 2 3 4 5 6 7 8 9 10 11 12 13  using Colors, ColorSchemes figure = (; resolution = (600, 400), font = \u0026#34;CMU Serif\u0026#34;) axis = (; xlabel = L\u0026#34;x\u0026#34;, ylabel = L\u0026#34;y\u0026#34;, aspect = DataAspect()) # cmap = ColorScheme(range(colorant\u0026#34;red\u0026#34;, colorant\u0026#34;green\u0026#34;, length = 3)) mycmap = ColorScheme([RGB{Float64}(i, 1.5i, 2i) for i in [0.0, 0.25, 0.35, 0.5]]) fig, ax, pltobj = heatmap(rand(-1:1, 20, 20); colormap = cgrad(mycmap, 3, categorical = true, rev = true), # cgrad and Symbol, mygrays, axis = axis, figure = figure) cbar = Colorbar(fig[1, 2], pltobj, label = \u0026#34;Categories\u0026#34;) cbar.ticks = ([-0.66, 0, 0.66], [\u0026#34;negative\u0026#34;, \u0026#34;neutral\u0026#34;, \u0026#34;positive\u0026#34;]) colsize!(fig.layout, 1, Aspect(1, 1.0)) fig   1 2 3 4 5 6 7 8  figure = (; resolution = (600, 400), font = \u0026#34;CMU Serif\u0026#34;) axis = (; xlabel = L\u0026#34;x\u0026#34;, ylabel = L\u0026#34;y\u0026#34;, aspect = DataAspect()) fig, ax, pltobj = heatmap(rand(20, 20); colorrange = (0, 1), colormap = [\u0026#34;red\u0026#34;, \u0026#34;black\u0026#34;], axis = axis, figure = figure) scatter!(ax, [11], [11], color = (\u0026#34;#C0C0C0\u0026#34;, 0.5), markersize = 150) Colorbar(fig[1, 2], pltobj, label = \u0026#34;2 colors\u0026#34;) colsize!(fig.layout, 1, Aspect(1, 1.0)) fig   Custom cycle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36  function new_cycle_theme() # https://nanx.me/ggsci/reference/pal_locuszoom.html  my_colors = [\u0026#34;#D43F3AFF\u0026#34;, \u0026#34;#EEA236FF\u0026#34;, \u0026#34;#5CB85CFF\u0026#34;, \u0026#34;#46B8DAFF\u0026#34;, \u0026#34;#357EBDFF\u0026#34;, \u0026#34;#9632B8FF\u0026#34;, \u0026#34;#B8B8B8FF\u0026#34;] cycle = Cycle([:color, :linestyle, :marker], covary = true) # alltogether my_markers = [:circle, :rect, :utriangle, :dtriangle, :diamond, :pentagon, :cross, :xcross] my_linestyle = [nothing, :dash, :dot, :dashdot, :dashdotdot] Theme(fontsize = 16, font = \u0026#34;CMU Serif\u0026#34;, colormap = :linear_bmy_10_95_c78_n256, palette = (color = my_colors, marker = my_markers, linestyle = my_linestyle), Lines = (cycle = cycle,), Scatter = (cycle = cycle,), Axis = (xlabelsize = 20, xgridstyle = :dash, ygridstyle = :dash, xtickalign = 1, ytickalign = 1, yticksize = 10, xticksize = 10, xlabelpadding = -5, xlabel = \u0026#34;x\u0026#34;, ylabel = \u0026#34;y\u0026#34;), Legend = (framecolor = (:black, 0.5), bgcolor = (:white, 0.5)), Colorbar = (ticksize = 16, tickalign = 1, spinewidth = 0.5)) end function scatters_and_lines() x = collect(0:10) xh = LinRange(4, 6, 25) yh = LinRange(70, 95, 25) h = randn(25, 25) fig = Figure(resolution = (600, 400), font = \u0026#34;CMU Serif\u0026#34;) ax = Axis(fig[1, 1], xlabel = L\u0026#34;x\u0026#34;, ylabel = L\u0026#34;f(x,a)\u0026#34;) for i in x lines!(ax, x, i .* x; label = latexstring(\u0026#34;$(i)x\u0026#34;)) scatter!(ax, x, i .* x; markersize = 13, strokewidth = 0.25, label = latexstring(\u0026#34;$(i)x\u0026#34;)) end hm = heatmap!(xh, yh, h) axislegend(L\u0026#34;f(x)\u0026#34;; merge = true, position = :lt, nbanks = 2, labelsize = 14) Colorbar(fig[1, 2], hm, label = \u0026#34;new default colormap\u0026#34;) limits!(ax, -0.5, 10.5, -5, 105) colgap!(fig.layout, 5) fig end with_theme(scatters_and_lines, new_cycle_theme())   Layouts 1 2 3 4 5 6 7 8 9 10 11 12 13  function first_layout() seed!(123) x, y, z = randn(6), randn(6), randn(6) fig = Figure(resolution = (600, 400), backgroundcolor = :grey90) ax = Axis(fig[1, 1], backgroundcolor = :white) pltobj = scatter!(ax, x, y; color = z, label=\u0026#34;scatters\u0026#34;) lines!(ax, x, 1.1y; label = \u0026#34;line\u0026#34;) Legend(fig[2, 1:2], ax, \u0026#34;labels\u0026#34;, orientation = :horizontal) Colorbar(fig[1, 2], pltobj, label = \u0026#34;colorbar\u0026#34;) fig end first_layout()   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  function first_layout_fixed() seed!(123) x, y, z = randn(6), randn(6), randn(6) fig = Figure(figure_padding = (0, 3, 5, 2), resolution = (600, 400), backgroundcolor = :grey90, font = \u0026#34;CMU Serif\u0026#34;) ax = Axis(fig[1, 1], xlabel = L\u0026#34;x\u0026#34;, ylabel = L\u0026#34;y\u0026#34;, title = \u0026#34;Layout example\u0026#34;, backgroundcolor = :white) pltobj = scatter!(ax, x, y; color = z, label = \u0026#34;scatters\u0026#34;) lines!(ax, x, 1.1y, label = \u0026#34;line\u0026#34;) Legend(fig[2, 1:2], ax, \u0026#34;Labels\u0026#34;, orientation = :horizontal, tellheight = true, titleposition = :left) Colorbar(fig[1, 2], pltobj, label = \u0026#34;colorbar\u0026#34;) # additional aesthetics  Box(fig[1, 1, Right()], color = (:slateblue1, 0.35)) Label(fig[1, 1, Right()], \u0026#34;protrusion\u0026#34;, fontsize = 18, rotation = pi / 2, padding = (3, 3, 3, 3)) Label(fig[1, 1, TopLeft()], \u0026#34;(a)\u0026#34;, fontsize = 18, padding = (0, 3, 8, 0)) colgap!(fig.layout, 5) rowgap!(fig.layout, 5) fig end first_layout_fixed()   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  function complex_layout_double_axis() seed!(123) x = LinRange(0, 1, 10) y = LinRange(0, 1, 10) z = rand(10, 10) fig = Figure(resolution = (600, 400), font = \u0026#34;CMU Serif\u0026#34;, backgroundcolor = :grey90) ax1 = Axis(fig, xlabel = L\u0026#34;x\u0026#34;, ylabel=L\u0026#34;y\u0026#34;) ax2 = Axis(fig, xlabel = L\u0026#34;x\u0026#34;) heatmap!(ax1, x, y, z; colorrange = (0, 1)) series!(ax2, abs.(z[1:4, :]); labels = [\u0026#34;lab $i\u0026#34; for i = 1:4], color = :Set1_4) hm = scatter!(10x, y; color = z[1, :], label= \u0026#34;dots\u0026#34;, colorrange = (0, 1)) hideydecorations!(ax2, ticks = false, grid = false) linkyaxes!(ax1, ax2) #layout fig[1, 1] = ax1 fig[1, 2] = ax2 Label(fig[1, 1, TopLeft()], \u0026#34;(a)\u0026#34;, textsize = 18, padding = (0, 6, 8, 0)) Label(fig[1, 2, TopLeft()], \u0026#34;(b)\u0026#34;, textsize = 18, padding = (0, 6, 8, 0)) Colorbar(fig[2, 1:2], hm, label = \u0026#34;colorbar\u0026#34;, vertical = false, flipaxis = false) Legend(fig[1, 3], ax2, \u0026#34;Legend\u0026#34;) colgap!(fig.layout, 5) rowgap!(fig.layout, 5) fig end complex_layout_double_axis()   1 2 3 4 5 6 7 8 9 10 11 12 13 14  function squares_layout() seed!(123) letters = reshape(collect(\u0026#39;a\u0026#39;:\u0026#39;d\u0026#39;), (2, 2)) fig = Figure(resolution = (600, 400), fontsize = 14, font = \u0026#34;CMU Serif\u0026#34;, backgroundcolor = :grey90) axs = [Axis(fig[i, j], aspect = DataAspect()) for i = 1:2, j = 1:2] hms = [heatmap!(axs[i, j], randn(10, 10), colorrange = (-2, 2)) for i = 1:2, j = 1:2] Colorbar(fig[1:2, 3], hms[1], label = \u0026#34;colorbar\u0026#34;) [Label(fig[i, j, TopLeft()], \u0026#34;($(letters[i, j]))\u0026#34;, textsize = 16, padding = (-2, 0, -20, 0)) for i = 1:2, j = 1:2] colgap!(fig.layout, 5) rowgap!(fig.layout, 5) fig end squares_layout()   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  using Dates function mixed_mode_layout() seed!(123) longlabels = [\u0026#34;$(today() − Day(1))\u0026#34;, \u0026#34;$(today())\u0026#34;, \u0026#34;$(today() + Day(1))\u0026#34;] fig = Figure(resolution = (600, 400), fontsize = 12, backgroundcolor = :grey90, font = \u0026#34;CMU Serif\u0026#34;) ax1 = Axis(fig[1, 1]) ax2 = Axis(fig[1, 2], xticklabelrotation = pi / 2, alignmode = Mixed(bottom = 0), xticks = ([1, 5, 10], longlabels)) ax3 = Axis(fig[2, 1:2]) ax4 = Axis(fig[3, 1:2]) axs = [ax1, ax2, ax3, ax4] [lines!(ax, 1:10, rand(10)) for ax in axs] hidexdecorations!(ax3; ticks = false, grid = false) Box(fig[2:3, 1:2, Right()], color = (:slateblue1, 0.35)) Label(fig[2:3, 1:2, Right()], \u0026#34;protrusion\u0026#34;, rotation = pi / 2, textsize = 14, padding = (3, 3, 3, 3)) Label(fig[1, 1:2, Top()], \u0026#34;Mixed alignmode\u0026#34;, textsize = 16, padding = (0, 0, 15, 0)) colsize!(fig.layout, 1, Auto(2)) rowsize!(fig.layout, 2, Auto(0.5)) rowsize!(fig.layout, 3, Auto(0.5)) rowgap!(fig.layout, 1, 15) rowgap!(fig.layout, 2, 0) colgap!(fig.layout, 5) fig end mixed_mode_layout()   Nested Axis (subplots) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  function nested_sub_plot!(fig) color = rand(RGBf) ax1 = Axis(fig[1, 1], backgroundcolor = (color, 0.25)) ax2 = Axis(fig[1, 2], backgroundcolor = (color, 0.25)) ax3 = Axis(fig[2, 1:2], backgroundcolor = (color, 0.25)) ax4 = Axis(fig[1:2, 3], backgroundcolor = (color, 0.25)) return (ax1, ax2, ax3, ax4) end function main_figure() fig = Figure() Axis(fig[1, 1]) nested_sub_plot!(fig[1, 2]) nested_sub_plot!(fig[1, 3]) nested_sub_plot!(fig[2, 1:3]) fig end main_figure()   Nested GridLayout 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  function nested_Grid_Layouts() fig = Figure(backgroundcolor = RGBf(0.96, 0.96, 0.96)) ga = fig[1, 1] = GridLayout() gb = fig[1, 2] = GridLayout() gc = fig[1, 3] = GridLayout() gd = fig[2, 1:3] = GridLayout() gA = Axis(ga[1, 1]) nested_sub_plot!(gb) axsc = nested_sub_plot!(gc) nested_sub_plot!(gd) [hidedecorations!(axsc[i], grid = false, ticks = false) for i = 1:length(axsc)] colgap!(gc, 5) rowgap!(gc, 5) rowsize!(fig.layout, 2, Auto(0.5)) colsize!(fig.layout, 1, Auto(0.5)) fig end nested_Grid_Layouts()   Inset plots 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  function add_box_inset(fig; left = 100, right = 250, bottom = 200, top = 300, bgcolor = :grey90) inset_box = Axis(fig, bbox = BBox(left, right, bottom, top), xticklabelsize = 12, yticklabelsize = 12, backgroundcolor = bgcolor) # bring content upfront  translate!(inset_box.scene, 0, 0, 10) elements = keys(inset_box.elements) filtered = filter(ele -\u0026gt; ele != :xaxis \u0026amp;\u0026amp; ele != :yaxis, elements) foreach(ele -\u0026gt; translate!(inset_box.elements[ele], 0, 0, 9), filtered) return inset_box end function figure_box_inset() fig = Figure(resolution = (650, 400)) ax = Axis(fig[1, 1], backgroundcolor = :white) inset_ax1 = add_box_inset(fig; left = 100, right = 250, bottom = 200, top = 300, bgcolor = :grey90) inset_ax2 = add_box_inset(fig; left = 500, right = 600, bottom = 100, top = 200, bgcolor = (:white, 0.65)) lines!(ax, 1:10) lines!(inset_ax1, 1:10) scatter!(inset_ax2, 1:10, color = :black) fig end figure_box_inset()   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  function add_axis_inset(; pos = fig[1, 1], halign = 0.1, valign = 0.5,width = Relative(0.5), height = Relative(0.35), bgcolor = :lightgray) inset_box = Axis(pos, width = width, height = height, halign = halign, valign = valign, xticklabelsize = 12, yticklabelsize = 12, backgroundcolor = bgcolor) # bring content upfront translate!(inset_box.scene, 0, 0, 10) return inset_box end function figure_axis_inset() fig = Figure(resolution = (600, 400)) ax = Axis(fig[1, 1], backgroundcolor = :white) inset_ax1 = add_axis_inset(; pos = fig[1, 1], halign = 0.1, valign = 0.65, width=Relative(0.3), height = Relative(0.3), bgcolor = :grey90) inset_ax2 = add_axis_inset(; pos = fig[1, 1], halign = 1, valign = 0.25, width = Relative(0.25), height = Relative(0.3), bgcolor = (:white, 0.65)) lines!(ax, 1:10) lines!(inset_ax1, 1:10) scatter!(inset_ax2, 1:10, color = :black) fig end figure_axis_inset()   GLMakie.jl Scatters and Lines 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  using GLMakie GLMakie.activate!() function scatters_in_3D() seed!(123) xyz = randn(10, 3) x, y, z = xyz[:, 1], xyz[:, 2], xyz[:, 3] fig = Figure(resolution = (1200, 400)) ax1 = Axis3(fig[1, 1]; aspect = (1, 1, 1), perspectiveness = 0.5) ax2 = Axis3(fig[1, 2]; aspect = (1, 1, 1), perspectiveness = 0.5) ax3 = Axis3(fig[1, 3]; aspect = :data, perspectiveness = 0.5) scatter!(ax1, x, y, z; markersize = 50) meshscatter!(ax2, x, y, z; markersize = 0.25) hm = meshscatter!(ax3, x, y, z; markersize = 0.25, marker = FRect3D(Vec3f0(0), Vec3f0(1)), color = 1:size(xyz)[2], colormap = :plasma, transparency = false) Colorbar(fig[1, 4], hm, label = \u0026#34;values\u0026#34;, height = Relative(0.5)) fig end scatters_in_3D()   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  function lines_in_3D() seed!(123) xyz = randn(10, 3) x, y, z = xyz[:, 1], xyz[:, 2], xyz[:, 3] fig = Figure(resolution = (1300, 400)) ax1 = Axis3(fig[1, 1]; aspect = (1, 1, 1), perspectiveness = 0.5) ax2 = Axis3(fig[1, 2]; aspect = (1, 1, 1), perspectiveness = 0.5) ax3 = Axis3(fig[1, 3]; aspect = :data, perspectiveness = 0.5) lines!(ax1, x, y, z; color = 1:size(xyz)[2], linewidth = 3) scatterlines!(ax2, x, y, z; markersize = 50) hm = meshscatter!(ax3, x, y, z; markersize = 0.2, color = 1:size(xyz)[2]) lines!(ax3, x, y, z; color = 1:size(xyz)[2]) Colorbar(fig[2, 1], hm; label=\u0026#34;values\u0026#34;, height = 15, vertical = false, flipaxis = false, ticksize = 15, tickalign = 1, width = Relative(3.55 / 4)) fig end lines_in_3D()   Surfaces, wireframe, contour, contourf and contour3d 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  function peaks(; n = 49) x = LinRange(-3, 3, n) y = LinRange(-3, 3, n) a = 3 * (1 .- x\u0026#39;) .^ 2 .* exp.(-(x\u0026#39; .^ 2) .- (y .+ 1) .^ 2) b = 10 * (x\u0026#39; / 5 .- x\u0026#39; .^ 3 .- y .^ 5) .* exp.(-x\u0026#39; .^ 2 .- y .^ 2) c = 1 / 3 * exp.(-(x\u0026#39; .+ 1) .^ 2 .- y .^ 2) return (x, y, a .- b .- c) end function plot_peaks_function() x, y, z = peaks() x2, y2, z2 = peaks(; n = 15) fig = Figure(resolution = (1300, 400), fontsize = 26) axs = [Axis3(fig[1, i]; aspect = (1, 1, 1)) for i = 1:3] hm = surface!(axs[1], x, y, z) wireframe!(axs[2], x2, y2, z2) contour3d!(axs[3], x, y, z; levels = 20) Colorbar(fig[1, 4], hm, height = Relative(0.5)) fig end plot_peaks_function()   1 2 3 4 5 6 7 8 9 10 11 12  function heatmap_contour_and_contourf() x, y, z = peaks() fig = Figure(resolution = (1300, 400), fontsize = 26) axs = [Axis(fig[1, i]; aspect = DataAspect()) for i = 1:3] hm = heatmap!(axs[1], x, y, z) contour!(axs[2], x, y, z; levels = 20) contourf!(axs[3], x, y, z) Colorbar(fig[1, 4], hm, height = Relative(0.5)) fig end heatmap_contour_and_contourf()   1 2 3 4 5 6 7 8 9 10 11 12  function heatmap_contour_and_contourf_in_a_3d_plane() x, y, z = peaks() fig = Figure(resolution = (1600, 400), fontsize = 26) axs = [Axis3(fig[1, i]) for i = 1:3] hm = heatmap!(axs[1], x, y, z) contour!(axs[2], x, y, z; levels = 20) contourf!(axs[3], x, y, z) Colorbar(fig[1, 4], hm, height = Relative(0.5)) fig end heatmap_contour_and_contourf_in_a_3d_plane()   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  using TestImages function mixing_surface_contour3d_contour_and_contourf() img = testimage(\u0026#34;coffee.png\u0026#34;) x, y, z = peaks() cmap = :Spectral_11 fig = Figure(resolution = (1200, 600), fontsize = 26) ax1 = Axis3(fig[1, 1]; aspect = (1, 1, 1), elevation=pi / 6, xzpanelcolor=(:black, 0.75), perspectiveness = 0.5, yzpanelcolor = :black, zgridcolor = :grey70, ygridcolor = :grey70, xgridcolor = :grey70) ax2 = Axis3(fig[1, 3]; aspect = (1, 1, 1), elevation = pi / 6, perspectiveness = 0.5) hm = surface!(ax1, x, y, z; colormap = (cmap, 0.95), shading = true) contour3d!(ax1, x, y, z .+ 0.02; colormap = cmap, levels = 20, linewidth = 2) xmin, ymin, zmin = minimum(ax1.finallimits[]) xmax, ymax, zmax = maximum(ax1.finallimits[]) contour!(ax1, x, y, z; colormap = cmap, levels = 20, transformation = (:xy, zmax)) contourf!(ax1, x, y, z; colormap = cmap, transformation = (:xy, zmin)) Colorbar(fig[1, 2], hm, width = 15, ticksize = 15, tickalign = 1, height = Relative(0.35)) # transformations into planes  heatmap!(ax2, x, y, z; colormap = :viridis, transformation = (:yz, 3.5)) contourf!(ax2, x, y, z; colormap = :CMRmap, transformation = (:xy, -3.5)) contourf!(ax2, x, y, z; colormap = :bone_1, transformation = (:xz, 3.5)) image!(ax2, -3 .. 3, -3 .. 2, rotr90(img); transformation = (:xy, 3.8)) xlims!(ax2, -3.8, 3.8) ylims!(ax2, -3.8, 3.8) zlims!(ax2, -3.8, 3.8) fig end mixing_surface_contour3d_contour_and_contourf()   Arrows and Streamplots 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  using LinearAlgebra function arrows_and_streamplot_in_3d() ps = [Point3f0(x, y, z) for x = -3:1:3 for y = -3:1:3 for z = -3:1:3] ns = map(p -\u0026gt; 0.1 * rand() * Vec3f0(p[2], p[3], p[1]), ps) lengths = norm.(ns) flowField(x, y, z) = Point(-y + x * (-1 + x^2 + y^2)^2, x + y * (-1 + x^2 + y^2)^2, z + x * (y - z^2)) fig = Figure(resolution = (1200, 600), fontsize = 26) axs = [Axis3(fig[1, i]; aspect =( 1, 1, 1), perspectiveness = 0.5) for i = 1:2] arrows!(axs[1], ps, ns, color = lengths, arrowsize = Vec3f0(0.2, 0.2, 0.3), linewidth = 0.1) streamplot!(axs[2], flowField, -4 .. 4, -4 .. 4, -4 .. 4, colormap = :plasma, gridsize = (7, 7), arrow_size = 0.25, linewidth = 1) fig end arrows_and_streamplot_in_3d()   Meshes and Volumes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  using GeometryBasics function mesh_volume_contour() # mesh objects rectMesh = FRect3D(Vec3f0(-0.5), Vec3f0(1)) recmesh = GeometryBasics.mesh(rectMesh) sphere = Sphere(Point3f0(0), 1) # https://juliageometry.github.io/GeometryBasics.jl/stable/primitives/  spheremesh = GeometryBasics.mesh(Tesselation(sphere, 64)) # uses 64 for tesselation, a smoother sphere  colors = [rand() for v in recmesh.position] # cloud points for volume  x = y = z = 1:10 vals = randn(10, 10, 10) fig = Figure(resolution = (1200, 400)) axs = [Axis3(fig[1, i]; aspect = (1, 1, 1), perspectiveness = 0.5) for i = 1:3] mesh!(axs[1], recmesh; color = colors, colormap = :rainbow, shading = false) mesh!(axs[1], spheremesh; color = (:white, 0.25), transparency = true) volume!(axs[2], x, y, z, vals; colormap = Reverse(:plasma)) contour!(axs[3], x, y, z, vals; colormap = Reverse(:plasma)) fig end mesh_volume_contour()   1 2 3 4 5 6 7 8 9 10 11  using GeometryBasics, ColorSchemes seed!(123) spheresGrid = [Point3f0(i, j, k) for i in 1:2:10 for j in 1:2:10 for k in 1:2:10] colorSphere = [RGBA(i * 0.1, j * 0.1, k * 0.1, 0.75) for i in 1:2:10 for j in 1:2:10 for k in 1:2:10] spheresPlane = [Point3f0(i, j, k) for i in 1:2.5:20 for j in 1:2.5:10 for k in 1:2.5:4] cmap = get(colorschemes[:plasma], LinRange(0, 1, 50)) colorsPlane = cmap[rand(1:50,50)] rectMesh = FRect3D(Vec3f0(-1, -1, 2.1), Vec3f0(22, 11, 0.5)) recmesh = GeometryBasics.mesh(rectMesh) colors = [RGBA(rand(4)...) for v in recmesh.position]   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  function grid_spheres_and_rectangle_as_plate() fig = with_theme(theme_dark()) do fig = Figure(resolution = (1200, 600)) ax1 = Axis3(fig[1, 1]; aspect = :data, perspectiveness = 0.5, azimuth = 0.72) ax2 = Axis3(fig[1, 2]; aspect = :data, perspectiveness = 0.5) meshscatter!(ax1, spheresGrid; color = colorSphere, markersize = 1, shading = false) meshscatter!(ax2, spheresPlane; color = colorsPlane, markersize = 0.75, lightposition = Vec3f0(10, 5, 2), ambient = Vec3f0(0.95, 0.95, 0.95), backlight = 1.0f0) mesh!(recmesh; color = colors, colormap = :rainbow, shading = false) limits!(ax1, 0, 10, 0, 10, 0, 10) fig end fig end grid_spheres_and_rectangle_as_plate()   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  x, y, z = peaks(; n = 15) δx = (x[2] - x[1]) / 2 δy = (y[2] - y[1]) / 2 cbarPal = :Spectral_11 ztmp = (z .- minimum(z)) ./ (maximum(z .- minimum(z))) cmap = get(colorschemes[cbarPal], ztmp) cmap2 = reshape(cmap, size(z)) ztmp2 = abs.(z) ./ maximum(abs.(z)) .+ 0.15 function histogram_or_bars_in_3d() fig= Figure(resolution = (1200, 600), fontsize = 26) ax1 = Axis3(fig[1, 1]; aspect = (1, 1, 1), elevation = π/6, perspectiveness= 0.5) ax2 = Axis3(fig[1, 2]; aspect = (1, 1, 1), perspectiveness = 0.5) rectMesh = FRect3D(Vec3f0(-0.5, -0.5, 0), Vec3f0(1, 1, 1)) meshscatter!(ax1, x, y, 0 * z, marker = rectMesh, color = z[:], markersize = Vec3f0.(2δx, 2δy, z[:]), colormap = :Spectral_11, shading = false) limits!(ax1, -3.5, 3.5, -3.5, 3.5, -7.45, 7.45) meshscatter!(ax2, x, y, 0 * z, marker = rectMesh, color = z[:], markersize = Vec3f0.(2δx, 2δy, z[:]), colormap = (:Spectral_11, 0.25), shading = false, transparency = true) for (idx, i) in enumerate(x), (idy, j) in enumerate(y) rectMesh = FRect3D(Vec3f0(i - δx, j - δy, 0), Vec3f0(2δx, 2δy, z[idx, idy])) recmesh = GeometryBasics.mesh(rectMesh) lines!(ax2, recmesh; color = (cmap2[idx, idy], ztmp2[idx, idy])) end fig end histogram_or_bars_in_3d()   Filled Line and Band 1 2 3 4 5 6 7 8 9 10 11 12 13  function filled_line_and_linesegments_in_3D() xs = LinRange(-3, 3, 10) lower = [Point3f0(i, -i, 0) for i in LinRange(0, 3, 100)] upper = [Point3f0(i, -i, sin(i) * exp(-(i + i))) for i in range(0, 3, length = 100)] fig = Figure(resolution = (1200, 600)) axs = [Axis3(fig[1, i]; elevation = pi/6, perspectiveness=0.5) for i = 1:2] band!(axs[1], lower, upper, color = repeat(norm.(upper), outer = 2), colormap = :CMRmap) lines!(axs[1], upper, color = :black) linesegments!(axs[2], cos.(xs), xs, sin.(xs), linewidth = 5, color = 1:length(xs)) fig end filled_line_and_linesegments_in_3D()    Storopoli, Huijzer and Alonso (2021). Julia Data Science. https://juliadatascience.io. ISBN: 9798489859165.  ","description":"Some examples of Makie.jl visualization","id":15,"section":"blogs","tags":["Julia"],"title":"Data Visualization with Makie","uri":"https://www.metasphinx.com/blogs/2023/2023-01-11-julia_data_science/"},{"content":"Plots 1 2 3 4 5 6  # pkg\u0026gt; add Plots PyPlot GR UnicodePlots using Plots # pyplot() # PyPlot backend eq(d) = -7.65 * sind(d) + 9.87 * sind(2d + 206) plot(eq, 1:365)   UnicodePlots 1 2 3 4 5  # add UnicodePlots using UnicodePlots FirstLinePlot = lineplot([1, 2, 3, 7], [1, 2, -5, 7], title = \u0026#34;First Line Plot\u0026#34;, border = :dotted) UnicodePlots.savefig(FirstLinePlot, \u0026#34;FirstLinePlot.txt\u0026#34;)   1 2  FirstDensityPlot = densityplot(collect(1:100), randn(100), border = :dotted) UnicodePlots.savefig(FirstDensityPlot, \u0026#34;FirstDensityPlot.txt\u0026#34;)   VegaLite 1 2 3 4 5 6  # add VegaLite using VegaLite X = [\u0026#34;Monday\u0026#34;, \u0026#34;Tuesday\u0026#34;, \u0026#34;Wednesday\u0026#34;, \u0026#34;Thrusday\u0026#34;, \u0026#34;Friday\u0026#34;,\u0026#34;Saturday\u0026#34;,\u0026#34;Sunday\u0026#34;] Y = [11, 11, 15, 13, 12, 13, 10] P = pie(X, Y)   Winston 1 2 3 4 5 6 7 8 9  using Winston x = range(0, stop = 3pi, length = 100); c = cos.(x); s = sin.(x); p = FramedPlot(title = \u0026#34;Winston Graphics!\u0026#34;, xlabel = \u0026#34;\\\\Sigma x^2_i\u0026#34;, ylabel = \u0026#34;\\\\Theta_i\u0026#34;) add(p, FillBetween(x, c, x, s)) add(p, Curve(x, c, color = \u0026#34;black\u0026#34;)) add(p, Curve(x, s, color = \u0026#34;red\u0026#34;))   Gadfly 1 2 3 4 5  using Gadfly using RDatasets iris = dataset(\u0026#34;datasets\u0026#34;, \u0026#34;iris\u0026#34;); Gadfly.plot(iris, x = :SepalLength, y = :SepalWidth, Geom.point)   1  Gadfly.plot(iris, x = :SepalLength, y = :SepalWidth, Geom.point, Geom.line)   1  Gadfly.plot(iris, x = :SepalLength, y = :SepalWidth, color = :Species, Geom.point)   Compose 1 2 3 4  using Compose composition = compose(compose(context(), rectangle()), fill(\u0026#34;tomato\u0026#34;)) draw(SVG(\u0026#34;simple.svg\u0026#34;, 6cm, 6cm), composition)   1 2 3  composition = compose(context(), (context(), Compose.circle(), fill(\u0026#34;bisque\u0026#34;)), (context(), rectangle(), fill(\u0026#34;tomato\u0026#34;))) composition |\u0026gt; SVG(\u0026#34;simple2.svg\u0026#34;)   PyPlot 1 2 3 4 5 6 7  using Conda Conda.add(\u0026#34;matplotlib\u0026#34;) using PyPlot x = range(0; stop = 2*pi, length = 500); y = sin.(3 * x + 4 * cos.(2 * x)); PyPlot.plot(x, y, color = \u0026#34;blue\u0026#34;, linewidth = 1.0, linestyle = \u0026#34;--\u0026#34;)   1  surf(rand(20, 30))   Gaston 1 2 3 4 5  using PGFPlots R = Axis([Plots.Linear(x-\u0026gt;sin(3x)*exp(-0.3x), (0,8), legendentry = L\u0026#34;$\\sin(3x)*exp(-0.3x)$\u0026#34;), Plots.Linear(x-\u0026gt;sqrt(x)/(1+x^2), (0,8), legendentry = L\u0026#34;$\\sqrt{2x}/(1+x^2)$\u0026#34;)]) save(\u0026#34;Plot_LinearPGF.svg\u0026#34;, R)    Summary from《LEARN JULIA PROGRAMMING》  ","description":"Several commonly used Julia plotting packages","id":16,"section":"blogs","tags":["Julia"],"title":"Julia plotting packages","uri":"https://www.metasphinx.com/blogs/2023/2023-01-09-julia_graphics/"}]